# MyBiome Orchestration Guide

## Updated Folder Structure

```
back_end/
â”œâ”€â”€ src/                       # Core source code
â”‚   â”œâ”€â”€ data/                     # Core data management
â”‚   â”‚   â”œâ”€â”€ config.py                # Unified configuration management
â”‚   â”‚   â”œâ”€â”€ repositories.py          # Database abstraction layer
â”‚   â”‚   â”œâ”€â”€ validators.py            # Main validation system
â”‚   â”‚   â”œâ”€â”€ api_clients.py           # API clients (PubMed, PMC, Semantic Scholar, LLM)
â”‚   â”‚   â”œâ”€â”€ error_handler.py         # Circuit breaker & retry logic
â”‚   â”‚   â””â”€â”€ utils.py                 # Helper functions
â”‚   â”‚
â”‚   â”œâ”€â”€ paper_collection/            # Data collection pipeline
â”‚   â”‚   â”œâ”€â”€ database_manager.py      # Database schema & operations
â”‚   â”‚   â”œâ”€â”€ pubmed_collector.py      # PubMed API integration
â”‚   â”‚   â”œâ”€â”€ semantic_scholar_enrichment.py # S2 data enrichment
â”‚   â”‚   â”œâ”€â”€ fulltext_retriever.py    # PMC & Unpaywall fulltext
â”‚   â”‚   â””â”€â”€ paper_parser.py          # Content parsing & extraction
â”‚   â”‚
â”‚   â”œâ”€â”€ llm/                         # LLM processing pipeline
â”‚   â”‚   â”œâ”€â”€ dual_model_analyzer.py   # Core dual-model analysis (gemma2 + qwen2.5)
â”‚   â”‚   â”œâ”€â”€ prompt_service.py        # Prompt templates & management
â”‚   â”‚   â”œâ”€â”€ pipeline.py              # LLM processing pipeline
â”‚   â”‚   â”œâ”€â”€ pipeline_analyzer.py     # Pipeline analysis tools
â”‚   â”‚   â””â”€â”€ emerging_category_analyzer.py # New category detection
â”‚   â”‚
â”‚   â”œâ”€â”€ interventions/               # Intervention classification
â”‚   â”‚   â”œâ”€â”€ taxonomy.py              # Intervention type definitions
â”‚   â”‚   â”œâ”€â”€ category_validators.py   # Category-specific validation rules
â”‚   â”‚   â””â”€â”€ search_terms.py          # Search term definitions
â”‚   â”‚
â”‚   â”œâ”€â”€ action_functions/            # Modular pipeline operations
â”‚   â”‚   â””â”€â”€ [various action functions]
â”‚   â”‚
â”‚   â””â”€â”€ utils/                       # Utility functions
â”‚       â””â”€â”€ imports.py               # Import helpers
â”‚
â”œâ”€â”€ pipelines/                       # ğŸš€ MAIN EXECUTION SCRIPTS
â”‚   â”œâ”€â”€ paper_collector.py           # ğŸ“š Unified Paper Collection
â”‚   â”œâ”€â”€ llm_processor.py             # ğŸ¤– Unified LLM Processing
â”‚   â”œâ”€â”€ research_orchestrator.py     # ğŸ¯ Master Research Orchestrator
â”‚   â”œâ”€â”€ collect_process_validate.py  # Legacy full pipeline
â”‚   â”œâ”€â”€ reprocess_abstracts.py       # Abstract reprocessing utility
â”‚   â”œâ”€â”€ run_s2_enrichment.py         # S2 enrichment utility
â”‚   â””â”€â”€ test_new_scripts.py          # Testing framework
â”‚
â”œâ”€â”€ data_mining/                     # ğŸ”¬ ADVANCED ANALYTICS & PATTERN DISCOVERY
â”‚   â”œâ”€â”€ data_mining_orchestrator.py  # ğŸ“Š Data Mining Orchestrator
â”‚   â”œâ”€â”€ bayesian_scorer.py           # Bayesian evidence scoring
â”‚   â”œâ”€â”€ medical_knowledge_graph.py   # Multi-edge medical knowledge graph
â”‚   â”œâ”€â”€ biological_patterns.py       # Biological pattern recognition
â”‚   â”œâ”€â”€ condition_similarity_mapping.py # Condition similarity analysis
â”‚   â”œâ”€â”€ treatment_recommendation_engine.py # AI treatment recommendations
â”‚   â”œâ”€â”€ innovation_tracking_system.py # Emerging treatment tracking
â”‚   â”œâ”€â”€ research_gaps.py             # Research gap identification
â”‚   â”œâ”€â”€ power_combinations.py        # Intervention combination analysis
â”‚   â”œâ”€â”€ failed_interventions.py      # Failed treatment analysis
â”‚   â”œâ”€â”€ fundamental_functions.py     # Core analytical functions
â”‚   â”œâ”€â”€ correlation_consistency_checker.py # Data consistency analysis
â”‚   â””â”€â”€ review_correlations.py       # Correlation review tools
â”‚
â”œâ”€â”€ testing/                         # Testing & validation
â”œâ”€â”€ utilities/                       # Administrative tools
â””â”€â”€ [other directories...]
```

## ğŸ¯ Main Orchestration Files

### 1. **`pipelines/research_orchestrator.py`** - Master Research Workflow
**Purpose**: Coordinates complete research campaigns from data collection to final reports

**What it does**:
- Orchestrates the entire research pipeline: Collection â†’ Processing â†’ Validation â†’ Reporting
- Manages multi-condition research campaigns
- Coordinates between paper_collector.py and llm_processor.py
- Provides comprehensive session management across all phases
- Handles thermal protection and error recovery throughout the workflow

**When to use**:
- âœ… **Complete research workflows** (most common use case)
- âœ… **Multi-condition campaigns**
- âœ… **Overnight/unattended operations**
- âœ… **When you want everything automated**

**Usage Examples**:
```bash
# Complete research workflow for one condition
python research_orchestrator.py "inflammatory bowel disease" --papers 500

# Multi-condition overnight campaign
python research_orchestrator.py --conditions "ibs,gerd,crohns,colitis" --papers-per-condition 1000 --overnight

# Collection only (skip processing)
python research_orchestrator.py --conditions "diabetes" --collection-only

# Processing only (papers already collected)
python research_orchestrator.py --processing-only --limit 100

# Resume interrupted campaign
python research_orchestrator.py --resume

# Check campaign status
python research_orchestrator.py --status
```

**Key Features**:
- ğŸ”„ **4-Phase Workflow**: Collection â†’ Processing â†’ Validation â†’ Reporting
- ğŸŒ¡ï¸ **Thermal Protection**: Monitors GPU temperature throughout
- ğŸ’¾ **Cross-Phase Sessions**: Maintains state across all workflow phases
- ğŸ“Š **Comprehensive Reporting**: Generates detailed campaign reports
- ğŸ” **Auto-Restart**: Recovers from errors and continues operation

---

### 2. **`pipelines/paper_collector.py`** - Standalone Paper Collection
**Purpose**: Robust paper collection from PubMed and Semantic Scholar

**What it does**:
- Collects papers from PubMed with intelligent search strategies
- Performs Semantic Scholar enrichment (interleaved or traditional)
- Handles network failures with exponential backoff retry
- Manages collection sessions with progress tracking
- Supports multi-condition batch processing

**When to use**:
- âœ… **Only need paper collection** (no LLM processing)
- âœ… **Building up paper database** before processing
- âœ… **Testing collection strategies**
- âœ… **Want fine control over collection parameters**

**Usage Examples**:
```bash
# Single condition collection
python paper_collector.py "ibs" --max-papers 200

# Multi-condition collection campaign
python paper_collector.py --conditions "ibs,gerd,crohns" --target-per-condition 500

# Traditional mode (no interleaved S2)
python paper_collector.py "diabetes" --traditional-mode --max-papers 100

# Resume collection session
python paper_collector.py --resume

# Check collection status
python paper_collector.py --status
```

**Key Features**:
- ğŸŒ **Multi-Source Collection**: PubMed + Semantic Scholar + PMC + Unpaywall
- ğŸ”„ **Network Resilience**: Exponential backoff retry for API failures
- ğŸ“ˆ **Session Persistence**: Resume from interruptions
- ğŸ¯ **Smart Search**: Interleaved S2 discovery for 6x paper expansion
- ğŸ“Š **Progress Tracking**: Real-time collection statistics

---

### 3. **`pipelines/llm_processor.py`** - Standalone LLM Processing
**Purpose**: Robust LLM-based intervention extraction with thermal protection

**What it does**:
- Processes papers using dual-model analysis (gemma2:9b + qwen2.5:14b)
- Monitors GPU temperature with predictive cooling
- Manages memory usage and performs automatic cleanup
- Tracks processing sessions with detailed metrics
- Handles processing failures with retry logic

**When to use**:
- âœ… **Only need LLM processing** (papers already collected)
- âœ… **Reprocessing existing papers** with updated models
- âœ… **Want fine control over thermal/memory settings**
- âœ… **Testing LLM processing performance**

**Usage Examples**:
```bash
# Process all unprocessed papers
python llm_processor.py --all

# Process specific number with thermal limits
python llm_processor.py --limit 50 --max-temp 75 --batch-size 3

# Reprocess failed papers
python llm_processor.py --reprocess-failed

# Process overnight with thermal monitoring
python llm_processor.py --all --overnight --thermal-status

# Resume processing session
python llm_processor.py --resume

# Check thermal status
python llm_processor.py --thermal-status
```

**Key Features**:
- ğŸŒ¡ï¸ **Advanced Thermal Protection**: Real-time GPU monitoring with predictive cooling
- ğŸ§  **Dual-Model Analysis**: Uses both gemma2:9b and qwen2.5:14b for robust extraction
- ğŸ’¾ **Memory Management**: Automatic cleanup and optimization
- ğŸ“Š **Performance Metrics**: Detailed timing and throughput statistics
- ğŸ” **Session Recovery**: Resume from any interruption point

---

### 4. **`data_mining/data_mining_orchestrator.py`** - Advanced Analytics Orchestrator
**Purpose**: Orchestrates advanced data mining and pattern discovery operations

**What it does**:
- Builds medical knowledge graphs from extracted interventions
- Performs Bayesian evidence scoring and statistical analysis
- Generates treatment recommendations and identifies research gaps
- Analyzes intervention combinations and biological patterns
- Creates comprehensive analytical reports

**When to use**:
- âœ… **After data collection and processing is complete**
- âœ… **Want to discover hidden patterns in the data**
- âœ… **Need treatment recommendations and research insights**
- âœ… **Building knowledge graphs and analytical models**

**Usage Examples**:
```bash
# Complete data mining pipeline
python data_mining_orchestrator.py --all

# Specific analyses
python data_mining_orchestrator.py --knowledge-graph --bayesian-scoring --recommendations

# Target specific conditions
python data_mining_orchestrator.py --conditions "ibs,gerd" --all

# Resume data mining session
python data_mining_orchestrator.py --resume
```

## ğŸ”„ Recommended Workflow

### **For New Users (Complete Workflow)**:
```bash
# 1. Complete research campaign (collection + processing + analysis)
python pipelines/research_orchestrator.py "inflammatory bowel disease" --papers 500

# 2. Monitor progress
python pipelines/research_orchestrator.py --status

# 3. After completion, run data mining
python data_mining/data_mining_orchestrator.py --all --conditions "inflammatory bowel disease"
```

### **For Advanced Users (Staged Workflow)**:
```bash
# Stage 1: Collection
python pipelines/paper_collector.py --conditions "ibs,gerd,crohns" --target-per-condition 1000

# Stage 2: Processing (after collection completes)
python pipelines/llm_processor.py --all --max-temp 80

# Stage 3: Data Mining (after processing completes)
python data_mining/data_mining_orchestrator.py --all --conditions "ibs,gerd,crohns"
```

### **For Overnight Operations**:
```bash
# Complete overnight campaign
python pipelines/research_orchestrator.py --conditions "ibs,gerd,crohns,colitis,diabetes" --papers-per-condition 1000 --overnight --auto-restart

# Monitor remotely
python pipelines/research_orchestrator.py --status
python pipelines/research_orchestrator.py --thermal-monitor
```

## ğŸ› ï¸ Session Management

All orchestrators support:
- **Auto-save**: Progress saved every 60 seconds
- **Resume**: `--resume` continues from interruption points
- **Status**: `--status` shows real-time progress
- **Configuration**: `--config config.json` for complex setups
- **Error Recovery**: Automatic restart capabilities

## ğŸŒ¡ï¸ Thermal Protection

GPU-intensive operations include:
- Real-time temperature monitoring
- Predictive cooling algorithms
- Automatic processing pauses
- Configurable temperature thresholds
- Hardware protection safeguards

Use `--thermal-status` to monitor and `--max-temp` to set limits.

## ğŸ“Š Output Structure

```
back_end/
â”œâ”€â”€ collection_session.json         # Collection progress
â”œâ”€â”€ processing_session.json         # Processing progress
â”œâ”€â”€ orchestration_session.json      # Orchestration progress
â”œâ”€â”€ data_mining_session.json        # Data mining progress
â”œâ”€â”€ collection_results/             # Collection outputs
â”œâ”€â”€ processing_results/             # Processing outputs
â”œâ”€â”€ orchestration_results/          # Campaign reports
â””â”€â”€ data_mining_results/            # Analytics outputs
```

This unified structure eliminates redundancy while providing maximum flexibility for different research workflows.