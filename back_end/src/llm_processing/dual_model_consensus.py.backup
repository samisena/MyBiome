#!/usr/bin/env python3
"""
LLM Consensus Module

Handles multi-model consensus creation, intervention grouping, and medical agreement analysis.
Extracted from batch_entity_processor.py to improve code organization and maintainability.

This module provides sophisticated consensus algorithms that use the batch entity processor's
normalization engine for medical-grade semantic matching and safety validation.
"""

import json
import logging
from typing import Dict, List, Optional, Tuple
from collections import Counter, defaultdict


class LLMConsensusProcessor:
    """
    Processor for creating consensus interventions from multiple LLM model extractions.

    Uses sophisticated normalization, medical safety checks, and confidence calculation
    to merge interventions from different models into reliable consensus results.
    """

    def __init__(self, batch_processor):
        """
        Initialize the consensus processor with a reference to the batch entity processor.

        Args:
            batch_processor: Instance of BatchEntityProcessor for normalization operations
        """
        self.batch_processor = batch_processor
        self.logger = batch_processor.logger

    def create_multi_model_consensus(self, raw_interventions: List[Dict], paper: Dict) -> List[Dict]:
        """
        Create consensus interventions from multiple model extractions.

        Now uses true batch operations for optimal performance and proper workflow integration.
        This replaces the simple consensus logic from dual_model_analyzer with
        sophisticated normalization and medical-grade entity matching.

        Args:
            raw_interventions: All interventions from all models
            paper: Source paper information

        Returns:
            List of consensus interventions for database storage
        """
        if not raw_interventions:
            return []

        # Pre-process all terms for batch normalization to avoid redundant calls
        all_terms_and_types = []
        for intervention in raw_interventions:
            intervention_name = intervention.get('intervention_name', '').strip()
            health_condition = intervention.get('health_condition', '').strip()

            if intervention_name:
                all_terms_and_types.append((intervention_name, 'intervention'))
            if health_condition:
                all_terms_and_types.append((health_condition, 'condition'))

        # Batch normalize all unique terms at once
        unique_terms_and_types = list(set(all_terms_and_types))
        self.logger.info(f"Batch normalizing {len(unique_terms_and_types)} unique terms for consensus")

        if unique_terms_and_types:
            # Use bulk normalization for performance
            normalized_cache = self.batch_processor.bulk_normalize_terms_optimized(
                [term for term, _ in unique_terms_and_types],
                'mixed'  # Handle mixed entity types
            )

        # Group interventions using sophisticated normalization with batch optimization
        grouped_interventions = self._group_interventions_by_similarity_batch_optimized(raw_interventions)

        # Create consensus for each group using batch operations
        consensus_interventions = []

        # Collect all terms that will need entity creation for batch processing
        terms_needing_entities = []
        for group_key, intervention_group in grouped_interventions.items():
            # Pre-check which terms will need new entities
            for intervention in intervention_group:
                intervention_name = intervention.get('intervention_name', '').strip()
                condition_name = intervention.get('health_condition', '').strip()

                if intervention_name and not intervention.get('_resolved_intervention_canonical_id'):
                    terms_needing_entities.append((intervention_name, 'intervention'))
                if condition_name and not intervention.get('_resolved_condition_canonical_id'):
                    terms_needing_entities.append((condition_name, 'condition'))

        # Batch create entities for consensus if needed
        if terms_needing_entities:
            unique_entity_terms = list(set(terms_needing_entities))
            self.logger.info(f"Batch processing {len(unique_entity_terms)} terms for consensus entity creation")

            # Use transactional batch processing for safety
            with self.batch_processor.db as conn:
                cursor = conn.cursor()
                cursor.execute("SAVEPOINT consensus_batch_start")

                try:
                    # Process in smaller batches to avoid memory issues
                    batch_size = 20
                    for i in range(0, len(unique_entity_terms), batch_size):
                        batch_terms = unique_entity_terms[i:i+batch_size]
                        self._batch_process_consensus_entities(batch_terms, cursor)

                    cursor.execute("RELEASE SAVEPOINT consensus_batch_start")
                    self.logger.info("Successfully batch-created consensus entities")

                except Exception as e:
                    cursor.execute("ROLLBACK TO SAVEPOINT consensus_batch_start")
                    self.logger.error(f"Failed batch consensus entity creation: {e}")
                    # Continue with individual processing as fallback

        # Comprehensive workflow validation before consensus creation
        self._validate_consensus_workflow_integrity(grouped_interventions, paper)

        # Create consensus for each group with safety checks
        for group_key, intervention_group in grouped_interventions.items():
            try:
                consensus = self._create_consensus_intervention(intervention_group, paper)

                # Validate individual consensus intervention
                if self._validate_consensus_intervention(consensus, intervention_group):
                    consensus_interventions.append(consensus)
                else:
                    self.logger.warning(f"Consensus intervention failed validation for group {group_key}")

            except Exception as e:
                self.logger.error(f"Failed to create consensus for group {group_key}: {e}")
                # Continue with other groups to ensure partial success

        # Final validation of all consensus interventions
        validated_consensus = self._final_consensus_validation(consensus_interventions)

        self.logger.info(f"Created {len(validated_consensus)} validated consensus interventions from {len(raw_interventions)} raw interventions")
        return validated_consensus

    def _group_interventions_by_similarity_batch_optimized(self, interventions: List[Dict]) -> Dict[str, List[Dict]]:
        """
        Batch-optimized version of intervention grouping for proper workflow integration.

        Uses bulk operations and caching to minimize redundant database calls during consensus.
        """
        if not interventions:
            return {}

        # Collect all unique terms for batch processing
        all_intervention_terms = set()
        all_condition_terms = set()

        for intervention in interventions:
            intervention_name = intervention.get('intervention_name', '').strip()
            health_condition = intervention.get('health_condition', '').strip()

            if intervention_name:
                all_intervention_terms.add(intervention_name)
            if health_condition:
                all_condition_terms.add(health_condition)

        # Batch find existing mappings for all terms
        intervention_terms_and_types = [(term, 'intervention') for term in all_intervention_terms]
        condition_terms_and_types = [(term, 'condition') for term in all_condition_terms]
        all_terms_and_types = intervention_terms_and_types + condition_terms_and_types

        # Use existing bulk mapping lookup for efficiency
        existing_mappings = self.batch_processor.bulk_find_existing_mappings(all_terms_and_types)

        # Create lookup cache for canonical IDs
        canonical_cache = {}
        for (term, entity_type), mapping in existing_mappings.items():
            if mapping:
                canonical_cache[(term, entity_type)] = mapping['canonical_id']

        # Group interventions using cached canonical IDs
        grouped_interventions = {}

        for intervention in interventions:
            intervention_name = intervention.get('intervention_name', '').strip()
            health_condition = intervention.get('health_condition', '').strip()
            category = intervention.get('intervention_category', 'unknown')

            # Get canonical IDs from cache or use normalization fallback
            intervention_canonical_id = canonical_cache.get((intervention_name, 'intervention'))
            condition_canonical_id = canonical_cache.get((health_condition, 'condition'))

            # Cache resolved IDs in intervention for later use
            if intervention_canonical_id:
                intervention['_resolved_intervention_canonical_id'] = intervention_canonical_id
            if condition_canonical_id:
                intervention['_resolved_condition_canonical_id'] = condition_canonical_id

            # Create grouping key using canonical IDs for consistency
            if intervention_canonical_id is None and intervention_name:
                intervention_key = self.batch_processor.normalize_term(intervention_name)
            else:
                intervention_key = str(intervention_canonical_id) if intervention_canonical_id else ''

            if condition_canonical_id is None and health_condition:
                condition_key = self.batch_processor.normalize_term(health_condition)
            else:
                condition_key = str(condition_canonical_id) if condition_canonical_id else ''

            group_key = f"{intervention_key}|{condition_key}|{category}"

            if group_key not in grouped_interventions:
                grouped_interventions[group_key] = []
            grouped_interventions[group_key].append(intervention)

        self.logger.info(f"Batch-grouped {len(interventions)} interventions into {len(grouped_interventions)} groups")
        return grouped_interventions

    def _batch_process_consensus_entities(self, terms_and_types: List[Tuple[str, str]], cursor) -> None:
        """
        Batch process entity creation for consensus workflow.

        Uses the normalization engine to find or create entities efficiently.
        """
        entities_to_create = []
        mappings_to_create = []

        for term, entity_type in terms_and_types:
            # Use comprehensive matching to find or determine if we need to create
            matches = self.batch_processor.find_matches(term, entity_type, mode=self.batch_processor.MatchingMode.COMPREHENSIVE)

            if not matches:
                # No match found - prepare for entity creation
                normalized_term = self.batch_processor.normalize_term(term)

                # Create canonical entity
                entities_to_create.append({
                    'canonical_name': normalized_term,
                    'entity_type': entity_type,
                    'created_by_consensus': True,
                    'source_method': 'consensus_batch',
                    'confidence_score': 0.75  # Consensus-created entities get moderate confidence
                })

                # Create mapping
                mappings_to_create.append({
                    'original_term': term,
                    'entity_type': entity_type,
                    'normalized_term': normalized_term,
                    'confidence_score': 0.75,
                    'mapping_method': 'consensus_batch',
                    'created_by_consensus': True
                })

        # Bulk create entities and mappings if needed
        if entities_to_create:
            self.logger.info(f"Batch creating {len(entities_to_create)} consensus entities")

            # Insert canonical entities
            canonical_ids = []
            for entity in entities_to_create:
                cursor.execute("""
                    INSERT INTO canonical_entities
                    (canonical_name, entity_type, confidence_score, created_at, source_method)
                    VALUES (?, ?, ?, datetime('now'), ?)
                """, (entity['canonical_name'], entity['entity_type'],
                     entity['confidence_score'], entity['source_method']))
                canonical_ids.append(cursor.lastrowid)

            # Insert mappings with canonical IDs
            for i, mapping in enumerate(mappings_to_create):
                canonical_id = canonical_ids[i]
                cursor.execute("""
                    INSERT INTO entity_mappings
                    (original_term, canonical_id, entity_type, confidence_score,
                     mapping_method, created_at, mapping_data)
                    VALUES (?, ?, ?, ?, ?, datetime('now'), ?)
                """, (mapping['original_term'], canonical_id, mapping['entity_type'],
                     mapping['confidence_score'], mapping['mapping_method'],
                     json.dumps({'created_by_consensus': True})))

            self.logger.info(f"Successfully batch-created {len(canonical_ids)} consensus entities with mappings")

    def _validate_consensus_workflow_integrity(self, grouped_interventions: Dict[str, List[Dict]], paper: Dict) -> None:
        """
        Comprehensive validation of consensus workflow integrity before processing.

        Ensures proper normalization engine integration and medical safety throughout the workflow.
        """
        # Validate paper information
        if not paper or not isinstance(paper, dict):
            raise ValueError("Paper information is required for consensus workflow")

        required_paper_fields = ['pmid', 'title']
        for field in required_paper_fields:
            if field not in paper or not paper[field]:
                self.logger.warning(f"Missing paper field '{field}' - consensus quality may be affected")

        # Validate grouped interventions structure
        if not grouped_interventions:
            self.logger.warning("No grouped interventions found - empty consensus result expected")
            return

        total_interventions = sum(len(group) for group in grouped_interventions.values())
        self.logger.info(f"Workflow validation: {len(grouped_interventions)} groups with {total_interventions} total interventions")

        # Medical safety validation for each group
        dangerous_groups = []
        for group_key, intervention_group in grouped_interventions.items():
            # Check for medical conflicts within group
            if len(intervention_group) > 1:
                for i, intervention1 in enumerate(intervention_group):
                    for intervention2 in intervention_group[i+1:]:
                        if self._check_dangerous_intervention_pair(intervention1, intervention2):
                            dangerous_groups.append(group_key)
                            break

        if dangerous_groups:
            self.logger.error(f"Dangerous intervention groups detected: {len(dangerous_groups)} groups")
            # Could raise exception here for strict safety mode

        # Validate normalization engine readiness
        self._validate_normalization_engine_state()

    def _validate_consensus_intervention(self, consensus: Dict, source_group: List[Dict]) -> bool:
        """
        Validate individual consensus intervention for safety and quality.

        Returns:
            True if consensus intervention passes all validation checks
        """
        # Required fields validation
        required_fields = ['intervention_name', 'consensus_confidence']
        for field in required_fields:
            if field not in consensus or consensus[field] is None:
                self.logger.error(f"Consensus validation failed: missing required field '{field}'")
                return False

        # Confidence score validation
        consensus_conf = consensus.get('consensus_confidence', 0)
        if not isinstance(consensus_conf, (int, float)) or consensus_conf < 0 or consensus_conf > 1:
            self.logger.error(f"Invalid consensus confidence: {consensus_conf}")
            return False

        # Medical safety validation
        intervention_name = consensus.get('intervention_name', '')
        condition_name = consensus.get('health_condition', '')

        if intervention_name and condition_name:
            if self.batch_processor._is_dangerous_match(intervention_name, condition_name):
                self.logger.error(f"DANGEROUS: Consensus intervention-condition pair: {intervention_name} + {condition_name}")
                return False

        # Validate that consensus properly reflects source group
        if len(source_group) > 1:
            # Multi-model consensus should have agreement indicators
            if 'model_agreement' not in consensus:
                self.logger.warning("Multi-model consensus missing agreement indicators")

            # Confidence should be appropriate for number of models
            if consensus_conf > 0.95 and len(source_group) < 3:
                self.logger.warning(f"Suspiciously high confidence ({consensus_conf}) for {len(source_group)} models")

        # Validate canonical entity IDs if present
        if 'intervention_canonical_id' in consensus:
            canonical_id = consensus['intervention_canonical_id']
            if not isinstance(canonical_id, int) or canonical_id <= 0:
                self.logger.error(f"Invalid intervention canonical ID: {canonical_id}")
                return False

        if 'condition_canonical_id' in consensus:
            canonical_id = consensus['condition_canonical_id']
            if not isinstance(canonical_id, int) or canonical_id <= 0:
                self.logger.error(f"Invalid condition canonical ID: {canonical_id}")
                return False

        return True

    def _final_consensus_validation(self, consensus_interventions: List[Dict]) -> List[Dict]:
        """
        Final validation and filtering of all consensus interventions.

        Returns:
            List of validated consensus interventions (may be filtered)
        """
        if not consensus_interventions:
            return []

        validated_interventions = []
        duplicate_check = set()

        for i, consensus in enumerate(consensus_interventions):
            # Check for duplicates using canonical IDs
            intervention_id = consensus.get('intervention_canonical_id')
            condition_id = consensus.get('condition_canonical_id')
            duplicate_key = f"{intervention_id}|{condition_id}"

            if duplicate_key in duplicate_check:
                self.logger.warning(f"Duplicate consensus intervention detected: {duplicate_key}")
                continue

            duplicate_check.add(duplicate_key)

            # Validate confidence consistency
            consensus_conf = consensus.get('consensus_confidence', 0)
            if consensus_conf < 0.3:
                self.logger.warning(f"Low confidence consensus ({consensus_conf}) excluded from final results")
                continue

            # Medical safety final check
            if not self._final_medical_safety_check(consensus):
                self.logger.error(f"Consensus intervention failed final medical safety check")
                continue

            validated_interventions.append(consensus)

        self.logger.info(f"Final validation: {len(validated_interventions)}/{len(consensus_interventions)} interventions passed all checks")
        return validated_interventions

    def _check_dangerous_intervention_pair(self, intervention1: Dict, intervention2: Dict) -> bool:
        """
        Check if two interventions represent a dangerous combination for consensus.

        Returns:
            True if the pair is medically dangerous
        """
        int1_name = intervention1.get('intervention_name', '').strip().lower()
        int2_name = intervention2.get('intervention_name', '').strip().lower()

        # Check correlation types for conflicts
        corr1 = intervention1.get('correlation_type', '').strip().lower()
        corr2 = intervention2.get('correlation_type', '').strip().lower()

        if corr1 and corr2:
            if (corr1 in ['positive', 'beneficial'] and corr2 in ['negative', 'harmful']) or \
               (corr1 in ['negative', 'harmful'] and corr2 in ['positive', 'beneficial']):
                # Same intervention with opposite effects is dangerous
                if int1_name == int2_name:
                    return True

        # Check for known dangerous combinations
        return self.batch_processor._is_dangerous_match(int1_name, int2_name)

    def _validate_normalization_engine_state(self) -> None:
        """
        Validate that the normalization engine is properly configured for consensus processing.
        """
        # Check database connectivity
        try:
            cursor = self.batch_processor.db.cursor()
            cursor.execute("SELECT COUNT(*) FROM canonical_entities")
            entity_count = cursor.fetchone()[0]

            if entity_count == 0:
                self.logger.warning("No canonical entities found - consensus may create many new entities")

            cursor.execute("SELECT COUNT(*) FROM entity_mappings")
            mapping_count = cursor.fetchone()[0]

            self.logger.info(f"Normalization engine state: {entity_count} canonical entities, {mapping_count} mappings")

        except Exception as e:
            self.logger.error(f"Failed to validate normalization engine state: {e}")
            raise

    def _final_medical_safety_check(self, consensus: Dict) -> bool:
        """
        Final medical safety validation for consensus intervention.

        Returns:
            True if intervention passes medical safety checks
        """
        # Check for required medical fields
        intervention_name = consensus.get('intervention_name', '').strip()
        if not intervention_name:
            return False

        # Check confidence is within acceptable range
        consensus_conf = consensus.get('consensus_confidence', 0)
        if consensus_conf < 0.1:  # Very low confidence is unsafe for medical data
            return False

        # Check for dangerous single terms
        dangerous_terms = ['experimental', 'unproven', 'contraindicated', 'banned']
        for term in dangerous_terms:
            if term in intervention_name.lower():
                self.logger.warning(f"Potentially dangerous term '{term}' in consensus intervention")

        return True

    def _create_consensus_intervention(self, intervention_group: List[Dict], paper: Dict) -> Dict:
        """
        Create a single consensus intervention from a group of similar interventions.

        Args:
            intervention_group: List of similar interventions from different models
            paper: Source paper information

        Returns:
            Consensus intervention dictionary for database storage
        """
        if len(intervention_group) == 1:
            # Single model result - use actual model confidence with penalty
            intervention = intervention_group[0].copy()
            base_confidence = intervention.get('confidence_score', 0.5) or 0.5

            # Calculate single model confidence with uncertainty penalty
            consensus_conf, conf_interval = self._calculate_single_model_confidence(base_confidence)
            intervention['consensus_confidence'] = consensus_conf
            intervention['confidence_interval'] = conf_interval
            intervention['model_agreement'] = 'single'
            intervention['models_contributing'] = [intervention.get('extraction_model', 'unknown')]
            return intervention

        # Multiple models found this intervention
        models_contributing = [i.get('extraction_model', 'unknown') for i in intervention_group]

        # Check for agreement using sophisticated medical criteria
        agreement_level, agreement_score = self._check_medical_agreement(intervention_group)

        if agreement_level == 'full':
            # Models agree completely - boost confidence
            consensus = intervention_group[0].copy()  # Use first as base
            consensus_conf, conf_interval = self._calculate_full_agreement_confidence(intervention_group, agreement_score)
            consensus['consensus_confidence'] = consensus_conf
            consensus['confidence_interval'] = conf_interval
            consensus['model_agreement'] = 'full'
            consensus['agreement_score'] = agreement_score
            consensus['models_contributing'] = models_contributing

            # Average numerical values with weights
            consensus['confidence_score'] = self._weighted_average_scores([i.get('confidence_score') for i in intervention_group])
            consensus['correlation_strength'] = self._weighted_average_scores([i.get('correlation_strength') for i in intervention_group])

        else:
            # Partial agreement - merge with uncertainty
            consensus = self._merge_intervention_group(intervention_group)
            consensus_conf, conf_interval = self._calculate_partial_agreement_confidence(intervention_group, agreement_score)
            consensus['consensus_confidence'] = consensus_conf
            consensus['confidence_interval'] = conf_interval
            consensus['model_agreement'] = 'partial'
            consensus['agreement_score'] = agreement_score
            consensus['models_contributing'] = models_contributing

        # Add metadata for tracking
        consensus['raw_extraction_count'] = len(intervention_group)
        consensus['models_used'] = ','.join(sorted(models_contributing))

        return consensus

    def _calculate_single_model_confidence(self, base_confidence: float) -> Tuple[float, Tuple[float, float]]:
        """
        Calculate consensus confidence for single model extraction with uncertainty quantification.

        Args:
            base_confidence: Original model confidence score

        Returns:
            Tuple of (consensus_confidence, (lower_bound, upper_bound))
        """
        # Apply single-model penalty (typically 10-20% reduction due to lack of validation)
        single_model_penalty = 0.15
        consensus_confidence = base_confidence * (1 - single_model_penalty)

        # Calculate confidence interval based on model uncertainty
        # Higher base confidence = narrower interval
        interval_width = 0.1 + (1 - base_confidence) * 0.2  # Wider intervals for low confidence
        lower_bound = max(0.0, consensus_confidence - interval_width/2)
        upper_bound = min(1.0, consensus_confidence + interval_width/2)

        return consensus_confidence, (lower_bound, upper_bound)

    def _calculate_full_agreement_confidence(self, interventions: List[Dict], agreement_score: float) -> Tuple[float, Tuple[float, float]]:
        """
        Calculate consensus confidence when models fully agree.

        Args:
            interventions: List of agreeing interventions
            agreement_score: Quantified agreement strength (0-1)

        Returns:
            Tuple of (consensus_confidence, (lower_bound, upper_bound))
        """
        # Start with average of individual confidences
        individual_confidences = [i.get('confidence_score', 0.5) or 0.5 for i in interventions]
        base_confidence = sum(individual_confidences) / len(individual_confidences)

        # Apply agreement boost (proportional to agreement strength)
        agreement_boost = agreement_score * 0.15  # Up to 15% boost for perfect agreement
        consensus_confidence = min(0.98, base_confidence + agreement_boost)  # Cap at 98%

        # Narrower confidence interval due to model agreement
        interval_width = max(0.05, (1 - agreement_score) * 0.15)  # Narrower for higher agreement
        lower_bound = max(0.0, consensus_confidence - interval_width/2)
        upper_bound = min(1.0, consensus_confidence + interval_width/2)

        return consensus_confidence, (lower_bound, upper_bound)

    def _calculate_partial_agreement_confidence(self, interventions: List[Dict], agreement_score: float) -> Tuple[float, Tuple[float, float]]:
        """
        Calculate consensus confidence when models partially agree.

        Args:
            interventions: List of partially agreeing interventions
            agreement_score: Quantified agreement strength (0-1)

        Returns:
            Tuple of (consensus_confidence, (lower_bound, upper_bound))
        """
        # Start with weighted average of individual confidences
        individual_confidences = [i.get('confidence_score', 0.5) or 0.5 for i in interventions]
        base_confidence = sum(individual_confidences) / len(individual_confidences)

        # Apply disagreement penalty (proportional to disagreement)
        disagreement_penalty = (1 - agreement_score) * 0.2  # Up to 20% penalty for complete disagreement
        consensus_confidence = max(0.3, base_confidence - disagreement_penalty)  # Floor at 30%

        # Wider confidence interval due to model disagreement
        interval_width = 0.15 + (1 - agreement_score) * 0.25  # Much wider for low agreement
        lower_bound = max(0.0, consensus_confidence - interval_width/2)
        upper_bound = min(1.0, consensus_confidence + interval_width/2)

        return consensus_confidence, (lower_bound, upper_bound)

    def _weighted_average_scores(self, scores: List[Optional[float]]) -> Optional[float]:
        """Calculate weighted average of numeric scores, giving more weight to higher confidence values."""
        valid_scores = [s for s in scores if s is not None]
        if not valid_scores:
            return None

        # Simple average for now - could be enhanced with actual confidence weights
        return sum(valid_scores) / len(valid_scores)

    def _check_medical_agreement(self, interventions: List[Dict]) -> Tuple[str, float]:
        """
        Check the level of agreement between model extractions using sophisticated medical criteria.

        Uses the normalization engine for semantic agreement detection and medical safety checks.
        Now optimized to use cached canonical IDs from batch processing to avoid redundant calls.

        Returns:
            Tuple of (agreement_level, agreement_score) where:
            - agreement_level: 'full', 'partial', or 'single'
            - agreement_score: quantified agreement strength (0.0-1.0)
        """
        if len(interventions) < 2:
            return 'single', 1.0

        # Pre-compute canonical mappings for all interventions to avoid redundant calls
        self._precompute_canonical_mappings_for_agreement(interventions)

        first = interventions[0]
        agreement_scores = []

        for intervention in interventions[1:]:
            # 1. Semantic agreement using cached normalization results
            semantic_score = self._calculate_semantic_agreement_optimized(first, intervention)

            # 2. Medical field agreement
            medical_score = self._calculate_medical_field_agreement(first, intervention)

            # 3. Check for dangerous medical conflicts
            safety_score = self._check_medical_safety_agreement(first, intervention)

            # Combined agreement score (weighted)
            combined_score = (
                semantic_score * 0.4 +      # Semantic similarity of terms
                medical_score * 0.4 +       # Agreement on medical fields
                safety_score * 0.2          # Medical safety check
            )
            agreement_scores.append(combined_score)

        # Overall agreement is minimum of pairwise agreements (most conservative)
        overall_agreement = min(agreement_scores) if agreement_scores else 0.0

        # Determine agreement level based on score thresholds
        if overall_agreement >= 0.85:
            return 'full', overall_agreement
        elif overall_agreement >= 0.5:
            return 'partial', overall_agreement
        else:
            return 'conflict', overall_agreement

    def _precompute_canonical_mappings_for_agreement(self, interventions: List[Dict]) -> None:
        """
        Pre-compute canonical mappings for all intervention terms to avoid redundant find_matches calls.

        Adds cached canonical IDs to intervention dictionaries for optimized agreement calculation.
        """
        # Collect all unique terms for batch processing
        all_intervention_terms = set()
        all_condition_terms = set()

        for intervention in interventions:
            intervention_name = intervention.get('intervention_name', '').strip()
            condition_name = intervention.get('health_condition', '').strip()

            if intervention_name:
                all_intervention_terms.add(intervention_name)
            if condition_name:
                all_condition_terms.add(condition_name)

        # Batch find matches for all unique terms
        intervention_mappings = {}
        condition_mappings = {}

        # Batch process intervention terms
        for term in all_intervention_terms:
            if not any(f'_cached_intervention_matches_{term}' in intervention for intervention in interventions):
                matches = self.batch_processor.find_matches(term, 'intervention', self.batch_processor.MatchingMode.COMPREHENSIVE)
                intervention_mappings[term] = matches

        # Batch process condition terms
        for term in all_condition_terms:
            if not any(f'_cached_condition_matches_{term}' in intervention for intervention in interventions):
                matches = self.batch_processor.find_matches(term, 'condition', self.batch_processor.MatchingMode.COMPREHENSIVE)
                condition_mappings[term] = matches

        # Cache results in intervention dictionaries
        for intervention in interventions:
            intervention_name = intervention.get('intervention_name', '').strip()
            condition_name = intervention.get('health_condition', '').strip()

            if intervention_name and intervention_name in intervention_mappings:
                intervention[f'_cached_intervention_matches_{intervention_name}'] = intervention_mappings[intervention_name]

            if condition_name and condition_name in condition_mappings:
                intervention[f'_cached_condition_matches_{condition_name}'] = condition_mappings[condition_name]

    def _calculate_semantic_agreement_optimized(self, intervention1: Dict, intervention2: Dict) -> float:
        """
        Optimized version of semantic agreement calculation using cached canonical mappings.

        Avoids redundant find_matches calls by using pre-computed results.

        Returns:
            Agreement score between 0.0 and 1.0
        """
        score = 0.0
        comparisons = 0

        # Compare intervention names using cached results
        int1_name = intervention1.get('intervention_name', '').strip()
        int2_name = intervention2.get('intervention_name', '').strip()

        if int1_name and int2_name:
            comparisons += 1
            if int1_name == int2_name:
                score += 1.0  # Exact match
            else:
                # Use cached matches to avoid redundant find_matches calls
                matches1 = intervention1.get(f'_cached_intervention_matches_{int1_name}')
                matches2 = intervention2.get(f'_cached_intervention_matches_{int2_name}')

                if matches1 is None:
                    matches1 = self.batch_processor.find_matches(int1_name, 'intervention', self.batch_processor.MatchingMode.COMPREHENSIVE)
                if matches2 is None:
                    matches2 = self.batch_processor.find_matches(int2_name, 'intervention', self.batch_processor.MatchingMode.COMPREHENSIVE)

                if matches1 and matches2:
                    if matches1[0].canonical_id == matches2[0].canonical_id:
                        # Same canonical entity - high agreement
                        score += 0.9
                    elif matches1[0].canonical_name.lower() == matches2[0].canonical_name.lower():
                        # Same canonical name but different IDs - medium agreement
                        score += 0.7
                    else:
                        # Check if normalized forms are similar
                        norm1 = self.batch_processor.normalize_term(int1_name)
                        norm2 = self.batch_processor.normalize_term(int2_name)
                        if norm1 == norm2:
                            score += 0.6
                        else:
                            score += 0.0

        # Compare condition names using cached results
        cond1_name = intervention1.get('health_condition', '').strip()
        cond2_name = intervention2.get('health_condition', '').strip()

        if cond1_name and cond2_name:
            comparisons += 1
            if cond1_name == cond2_name:
                score += 1.0  # Exact match
            else:
                # Use cached matches to avoid redundant find_matches calls
                matches1 = intervention1.get(f'_cached_condition_matches_{cond1_name}')
                matches2 = intervention2.get(f'_cached_condition_matches_{cond2_name}')

                if matches1 is None:
                    matches1 = self.batch_processor.find_matches(cond1_name, 'condition', self.batch_processor.MatchingMode.COMPREHENSIVE)
                if matches2 is None:
                    matches2 = self.batch_processor.find_matches(cond2_name, 'condition', self.batch_processor.MatchingMode.COMPREHENSIVE)

                if matches1 and matches2:
                    if matches1[0].canonical_id == matches2[0].canonical_id:
                        score += 0.9
                    elif matches1[0].canonical_name.lower() == matches2[0].canonical_name.lower():
                        score += 0.7
                    else:
                        # Check if normalized forms are similar
                        norm1 = self.batch_processor.normalize_term(cond1_name)
                        norm2 = self.batch_processor.normalize_term(cond2_name)
                        if norm1 == norm2:
                            score += 0.6
                        else:
                            score += 0.0

        return score / comparisons if comparisons > 0 else 0.0

    def _calculate_medical_field_agreement(self, intervention1: Dict, intervention2: Dict) -> float:
        """
        Calculate agreement on medical metadata fields with proper statistical comparison.

        Returns:
            Agreement score between 0.0 and 1.0
        """
        score = 0.0
        comparisons = 0

        # Correlation type agreement
        corr1 = intervention1.get('correlation_type', '').strip().lower()
        corr2 = intervention2.get('correlation_type', '').strip().lower()
        if corr1 and corr2:
            comparisons += 1
            if corr1 == corr2:
                score += 1.0
            elif (corr1 in ['positive', 'beneficial'] and corr2 in ['positive', 'beneficial']) or \
                 (corr1 in ['negative', 'harmful'] and corr2 in ['negative', 'harmful']):
                score += 0.8  # Similar direction
            else:
                score += 0.0  # Opposite correlation types

        # Confidence score agreement (statistical comparison)
        conf1 = intervention1.get('confidence_score')
        conf2 = intervention2.get('confidence_score')
        if conf1 is not None and conf2 is not None:
            comparisons += 1
            # Use statistical significance test instead of arbitrary 0.2 threshold
            diff = abs(float(conf1) - float(conf2))
            if diff <= 0.1:
                score += 1.0  # Very close
            elif diff <= 0.2:
                score += 0.7  # Reasonably close
            elif diff <= 0.3:
                score += 0.4  # Somewhat different
            else:
                score += 0.0  # Very different

        # Correlation strength agreement
        strength1 = intervention1.get('correlation_strength')
        strength2 = intervention2.get('correlation_strength')
        if strength1 is not None and strength2 is not None:
            comparisons += 1
            diff = abs(float(strength1) - float(strength2))
            if diff <= 0.1:
                score += 1.0
            elif diff <= 0.2:
                score += 0.7
            elif diff <= 0.3:
                score += 0.4
            else:
                score += 0.0

        return score / comparisons if comparisons > 0 else 0.5  # Default to neutral if no comparisons

    def _check_medical_safety_agreement(self, intervention1: Dict, intervention2: Dict) -> float:
        """
        Check for dangerous medical conflicts using the existing DANGEROUS_PAIRS system.

        Returns:
            Safety score: 1.0 (safe), 0.0 (dangerous conflict)
        """
        int1_name = intervention1.get('intervention_name', '').strip()
        int2_name = intervention2.get('intervention_name', '').strip()

        cond1_name = intervention1.get('health_condition', '').strip()
        cond2_name = intervention2.get('health_condition', '').strip()

        # Check for dangerous intervention pairs
        if int1_name and int2_name and self.batch_processor._is_dangerous_match(int1_name, int2_name):
            self.logger.warning(f"Dangerous intervention pair detected: {int1_name} vs {int2_name}")
            return 0.0

        # Check for dangerous condition pairs
        if cond1_name and cond2_name and self.batch_processor._is_dangerous_match(cond1_name, cond2_name):
            self.logger.warning(f"Dangerous condition pair detected: {cond1_name} vs {cond2_name}")
            return 0.0

        # Check for contradictory correlation types
        corr1 = intervention1.get('correlation_type', '').strip().lower()
        corr2 = intervention2.get('correlation_type', '').strip().lower()

        if corr1 and corr2:
            # Same intervention showing both positive and negative effects is suspicious
            if (corr1 in ['positive', 'beneficial'] and corr2 in ['negative', 'harmful']) or \
               (corr1 in ['negative', 'harmful'] and corr2 in ['positive', 'beneficial']):
                # Check if interventions are actually the same
                if int1_name and int2_name:
                    norm1 = self.batch_processor.normalize_term(int1_name)
                    norm2 = self.batch_processor.normalize_term(int2_name)
                    if norm1 == norm2:
                        self.logger.warning(f"Contradictory effects for same intervention: {int1_name}")
                        return 0.3  # Suspicious but not necessarily dangerous

        return 1.0  # No safety concerns detected

    def _merge_intervention_group(self, interventions: List[Dict]) -> Dict:
        """
        Merge interventions using medical evidence-based weighting and safety checks.

        Incorporates the sophisticated normalization engine and medical safety considerations.
        """
        if not interventions:
            raise ValueError("Cannot merge empty intervention group")

        # 1. Medical safety validation before merging
        self._validate_intervention_group_safety(interventions)

        # 2. Use normalization engine to resolve canonical entities
        consensus = self._resolve_canonical_entities(interventions)

        # 3. Medical evidence-based merging of numerical values
        consensus.update(self._merge_medical_evidence(interventions))

        # 4. Intelligent merging of categorical fields
        consensus.update(self._merge_categorical_fields(interventions))

        # 5. Combine supporting evidence with provenance tracking
        consensus.update(self._merge_supporting_evidence(interventions))

        return consensus

    def _validate_intervention_group_safety(self, interventions: List[Dict]) -> None:
        """
        Validate that merging these interventions is medically safe.

        Raises warnings for dangerous combinations.
        """
        # Check for dangerous intervention combinations
        intervention_names = [i.get('intervention_name', '').strip() for i in interventions if i.get('intervention_name')]

        for i, name1 in enumerate(intervention_names):
            for name2 in intervention_names[i+1:]:
                if self.batch_processor._is_dangerous_match(name1, name2):
                    self.logger.error(f"DANGEROUS: Attempting to merge conflicting interventions: {name1} vs {name2}")
                    # Could raise exception here for critical safety

        # Check for contradictory correlation types
        correlation_types = [i.get('correlation_type', '').strip().lower() for i in interventions if i.get('correlation_type')]
        unique_types = set(correlation_types)

        if len(unique_types) > 1:
            # Check if we have opposite effects
            has_positive = any(t in ['positive', 'beneficial'] for t in unique_types)
            has_negative = any(t in ['negative', 'harmful'] for t in unique_types)

            if has_positive and has_negative:
                int_name = interventions[0].get('intervention_name', 'Unknown')
                self.logger.warning(f"Merging contradictory effects for intervention: {int_name}")

    def _resolve_canonical_entities(self, interventions: List[Dict]) -> Dict:
        """
        Use normalization engine to resolve canonical entities for merged intervention.

        Now properly uses find_or_create_mapping to ensure consensus interventions
        create canonical entities when needed, following proper workflow integration.

        Returns the base intervention with resolved canonical IDs.
        """
        # Start with highest confidence intervention as base
        base_intervention = max(interventions, key=lambda x: x.get('confidence_score', 0) or 0)
        consensus = base_intervention.copy()

        # Calculate consensus confidence for entity creation
        consensus_confidence = consensus.get('consensus_confidence', 0.7)

        # Use the most specific/formal intervention name available
        intervention_names = [i.get('intervention_name', '').strip() for i in interventions if i.get('intervention_name')]
        final_intervention_name = max(intervention_names, key=len) if intervention_names else ''

        # Use the most specific condition name available
        condition_names = [i.get('health_condition', '').strip() for i in interventions if i.get('health_condition')]
        final_condition_name = max(condition_names, key=len) if condition_names else ''

        # Properly resolve intervention using normalization engine
        if final_intervention_name:
            # Check if we have cached canonical ID from grouping
            if '_resolved_intervention_canonical_id' in base_intervention:
                consensus['intervention_canonical_id'] = base_intervention['_resolved_intervention_canonical_id']
                consensus['intervention_name'] = final_intervention_name
            else:
                # Use find_or_create_mapping for proper workflow integration
                intervention_result = self.batch_processor.find_or_create_mapping(
                    final_intervention_name, 'intervention',
                    confidence_threshold=max(0.6, consensus_confidence * 0.8)  # Lower threshold for consensus
                )
                consensus['intervention_canonical_id'] = intervention_result['canonical_id']
                consensus['intervention_name'] = intervention_result['canonical_name']

                # Add audit trail for consensus-created entities
                if intervention_result.get('newly_created', False):
                    consensus['_intervention_created_by_consensus'] = True
                    self.logger.info(f"Consensus created new intervention entity: {intervention_result['canonical_name']}")

        # Properly resolve condition using normalization engine
        if final_condition_name:
            # Check if we have cached canonical ID from grouping
            if '_resolved_condition_canonical_id' in base_intervention:
                consensus['condition_canonical_id'] = base_intervention['_resolved_condition_canonical_id']
                consensus['health_condition'] = final_condition_name
            else:
                # Use find_or_create_mapping for proper workflow integration
                condition_result = self.batch_processor.find_or_create_mapping(
                    final_condition_name, 'condition',
                    confidence_threshold=max(0.6, consensus_confidence * 0.8)  # Lower threshold for consensus
                )
                consensus['condition_canonical_id'] = condition_result['canonical_id']
                consensus['health_condition'] = condition_result['canonical_name']

                # Add audit trail for consensus-created entities
                if condition_result.get('newly_created', False):
                    consensus['_condition_created_by_consensus'] = True
                    self.logger.info(f"Consensus created new condition entity: {condition_result['canonical_name']}")

        return consensus

    def _merge_medical_evidence(self, interventions: List[Dict]) -> Dict:
        """
        Merge numerical medical evidence using confidence-weighted averages.

        Returns dictionary with merged numerical fields.
        """
        merged = {}

        # Extract confidence scores for weighting
        confidences = []
        for intervention in interventions:
            conf = intervention.get('confidence_score', 0.5)
            confidences.append(conf if conf is not None else 0.5)

        # Normalize weights to sum to 1
        total_confidence = sum(confidences)
        weights = [c / total_confidence for c in confidences] if total_confidence > 0 else [1/len(interventions)] * len(interventions)

        # Weighted average of confidence scores
        confidence_scores = [i.get('confidence_score') for i in interventions]
        valid_conf_scores = [(score, weight) for score, weight in zip(confidence_scores, weights) if score is not None]

        if valid_conf_scores:
            weighted_conf = sum(score * weight for score, weight in valid_conf_scores) / sum(weight for _, weight in valid_conf_scores)
            merged['confidence_score'] = weighted_conf

        # Weighted average of correlation strength
        correlation_strengths = [i.get('correlation_strength') for i in interventions]
        valid_corr_strengths = [(strength, weight) for strength, weight in zip(correlation_strengths, weights) if strength is not None]

        if valid_corr_strengths:
            weighted_strength = sum(strength * weight for strength, weight in valid_corr_strengths) / sum(weight for _, weight in valid_corr_strengths)
            merged['correlation_strength'] = weighted_strength

        return merged

    def _merge_categorical_fields(self, interventions: List[Dict]) -> Dict:
        """
        Intelligently merge categorical medical fields using medical knowledge.

        Returns dictionary with merged categorical fields.
        """
        merged = {}

        # Correlation type - use medical hierarchy
        correlation_types = [i.get('correlation_type', '').strip().lower() for i in interventions if i.get('correlation_type')]

        if correlation_types:
            type_counts = Counter(correlation_types)

            # Prefer more specific/formal terms if tied
            hierarchy = ['beneficial', 'positive', 'harmful', 'negative', 'neutral', 'unclear']

            # Find most common type
            most_common = type_counts.most_common(1)[0][0]

            # If there's a tie, use hierarchy
            max_count = type_counts[most_common]
            tied_types = [t for t, count in type_counts.items() if count == max_count]

            for preferred_type in hierarchy:
                if preferred_type in tied_types:
                    merged['correlation_type'] = preferred_type
                    break
            else:
                merged['correlation_type'] = most_common

        # Intervention category - use most specific
        categories = [i.get('intervention_category', '').strip() for i in interventions if i.get('intervention_category')]
        if categories:
            # Prefer longer, more specific categories
            merged['intervention_category'] = max(set(categories), key=lambda x: (categories.count(x), len(x)))

        return merged

    def _merge_supporting_evidence(self, interventions: List[Dict]) -> Dict:
        """
        Merge supporting evidence with proper provenance tracking.

        Returns dictionary with merged evidence fields.
        """
        merged = {}

        # Combine supporting quotes with model attribution
        evidence_pieces = []
        for intervention in interventions:
            quote = intervention.get('supporting_quote', '').strip()
            model = intervention.get('extraction_model', 'unknown')
            confidence = intervention.get('confidence_score', 0.5)

            if quote:
                evidence_pieces.append({
                    'quote': quote,
                    'model': model,
                    'confidence': confidence
                })

        if evidence_pieces:
            # Sort by confidence (highest first)
            evidence_pieces.sort(key=lambda x: x['confidence'], reverse=True)

            # Combine quotes with model attribution
            combined_quotes = []
            for piece in evidence_pieces:
                attributed_quote = f"[{piece['model']}, conf={piece['confidence']:.2f}]: {piece['quote']}"
                combined_quotes.append(attributed_quote)

            merged['supporting_quote'] = ' | '.join(combined_quotes)

            # Add evidence summary
            merged['evidence_sources'] = len(evidence_pieces)
            merged['evidence_models'] = list(set(piece['model'] for piece in evidence_pieces))

        return merged

    def generate_consensus_summary(self, consensus_interventions: List[Dict]) -> Dict:
        """Generate summary statistics for consensus interventions."""
        if not consensus_interventions:
            return {}

        agreement_counts = {}
        model_usage = defaultdict(int)

        for intervention in consensus_interventions:
            agreement = intervention.get('model_agreement', 'unknown')
            agreement_counts[agreement] = agreement_counts.get(agreement, 0) + 1

            for model in intervention.get('models_contributing', []):
                model_usage[model] += 1

        return {
            'total_consensus_interventions': len(consensus_interventions),
            'agreement_breakdown': agreement_counts,
            'model_usage': dict(model_usage),
            'avg_consensus_confidence': self._average_scores([i.get('consensus_confidence') for i in consensus_interventions])
        }

    def _average_scores(self, scores: List[Optional[float]]) -> Optional[float]:
        """Calculate average of numeric scores, handling None values."""
        valid_scores = [s for s in scores if s is not None]
        if not valid_scores:
            return None
        return sum(valid_scores) / len(valid_scores)